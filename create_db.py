import os
import re
import sys
import csv
import json
import argparse
import hashlib
import logging
import pdfplumber
from datetime import datetime
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document

# ==== æ—¥èªŒè¨­å®š ====
LOG_DIR = "./logs"
os.makedirs(LOG_DIR, exist_ok=True)
now = datetime.now().strftime("%Y%m%d_%H%M%S")
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(f"{LOG_DIR}/create_db_{now}.log", encoding="utf-8"),
        logging.StreamHandler(sys.stdout)
    ]
)

# ==== åƒæ•¸èˆ‡ç›®éŒ„ ====
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_PATH = os.path.join(BASE_DIR, 'data')
VECTORSTORE_DIR = os.path.join(BASE_DIR, 'vectorstore')
PREVIEW_PATH = os.path.join(DATA_PATH, 'preview')
TABLES_PATH = os.path.join(DATA_PATH, 'tables')
CHUNKS_PATH = os.path.join(DATA_PATH, 'chunks')
os.makedirs(PREVIEW_PATH, exist_ok=True)
os.makedirs(TABLES_PATH, exist_ok=True)
os.makedirs(CHUNKS_PATH, exist_ok=True)

CHUNK_SIZE = 700
CHUNK_OVERLAP = 100
KEYWORDS_PATH = os.path.join(BASE_DIR, 'keywords.txt')
DEFAULT_KEYWORDS = [
    "ç‡Ÿæ¥­æ”¶å…¥", "ç§Ÿè³ƒè² å‚µ", "ç¾é‡‘åŠç´„ç•¶ç¾é‡‘", "å…¬å¸å‚µ", "ä½¿ç”¨æ¬Šè³‡ç”¢", "å­˜è²¨",
    "åœŸåœ°", "å»ºç¯‰ç‰©", "è‚¡æ±æ¬Šç›Š", "ç¸½è³‡ç”¢", "ç¸½è² å‚µ", "é‡‘èè³‡ç”¢", "æ¬Šç›Šæ³•æŠ•è³‡", "è³‡æœ¬æ”¯å‡º"
]

# ==== å®‰å…¨è³‡æ–™å¤¾å (å¸¶hash+timestampé˜²æ­¢é‡å) ====
def safe_folder_name(company):
    ascii_name = re.sub(r"[^\w]", "_", company)
    ascii_name = re.sub(r"[^a-zA-Z0-9_]", "_", ascii_name)
    hash_part = hashlib.md5(company.encode("utf-8")).hexdigest()[:8]
    return f"{ascii_name.lower()}_{hash_part}"         # å”¯ä¸€keyï¼Œæ°¸é ä¸€æ¨£


# ==== åŒæ­¥ company_mapping.json ====
def update_company_mapping(safe_folder, company_name):
    mapping_path = os.path.join(BASE_DIR, "company_mapping.json")
    try:
        mapping = {}
        if os.path.exists(mapping_path):
            with open(mapping_path, "r", encoding="utf-8") as f:
                mapping = json.load(f)
        mapping[safe_folder] = company_name
        with open(mapping_path, "w", encoding="utf-8") as f:
            json.dump(mapping, f, ensure_ascii=False, indent=2)
        logging.info(f"âœ… company_mapping.json æ›´æ–°æˆåŠŸ: {safe_folder} â†’ {company_name}")
    except Exception as e:
        logging.error(f"âŒ company_mapping.json å¯«å…¥éŒ¯èª¤: {e}")

def normalize_currency(text):
    text = re.sub(r'(NT\$|æ–°è‡ºå¹£|å°å¹£|å…ƒæ•´|TWD|å°å¹£)', 'æ–°å°å¹£', text)
    text = re.sub(r'([0-9,]+)\s*å„„å…ƒ', lambda m: str(int(m.group(1).replace(',',''))*1000000) + 'ä»Ÿå…ƒ', text)
    text = re.sub(r'([0-9,]+)\s*ç™¾è¬', lambda m: str(int(m.group(1).replace(',',''))*1000) + 'ä»Ÿå…ƒ', text)
    text = re.sub(r'([0-9,]+)\s*åƒè¬', lambda m: str(int(m.group(1).replace(',',''))*10000) + 'ä»Ÿå…ƒ', text)
    text = re.sub(r'([0-9,]+)\s*è¬å…ƒ', lambda m: str(int(m.group(1).replace(',',''))*10) + 'ä»Ÿå…ƒ', text)
    text = re.sub(r'([0-9,]+)\s*å…ƒ', lambda m: str(int(m.group(1).replace(',',''))//1000) + 'ä»Ÿå…ƒ', text)
    return text

def load_keywords():
    if os.path.exists(KEYWORDS_PATH):
        with open(KEYWORDS_PATH, "r", encoding="utf-8") as f:
            kws = [line.strip() for line in f if line.strip()]
            logging.info(f"è¼‰å…¥è‡ªå®šç¾©é—œéµå­—ï¼š{kws}")
            return kws
    else:
        logging.info("æœªåµæ¸¬åˆ°è‡ªå®šç¾©é—œéµå­—ï¼Œä½¿ç”¨é è¨­")
        return DEFAULT_KEYWORDS

def smart_split(text):
    return [s.strip() for s in re.split(r'(?:(?:\n{2,})|(?:\n\s*(?:[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]ã€|[IVXLCDM]+\.)\s+))', text) if s.strip()]

def extract_text_from_pdf(pdf_path):
    texts = []
    tables_data = []
    with pdfplumber.open(pdf_path) as pdf:
        for page_num, page in enumerate(pdf.pages):
            page_text = page.extract_text() or ""
            page_text = normalize_currency(page_text)
            texts.append({
                "content": page_text.strip(),
                "page": page_num+1,
                "type": "text"
            })
            tables = page.extract_tables()
            for table_idx, table in enumerate(tables):
                table_rows = []
                for row in table:
                    if row and any(row):
                        row_str = " | ".join([cell.strip() if cell else "" for cell in row])
                        row_str = normalize_currency(row_str)
                        table_rows.append(row_str)
                        tables_data.append({"page": page_num+1, "row": row})
                if table_rows:
                    texts.append({
                        "content": "\n".join(table_rows),
                        "page": page_num+1,
                        "type": "table"
                    })
                    csv_name = f"{os.path.splitext(os.path.basename(pdf_path))[0]}_p{page_num+1}_tb{table_idx+1}.csv"
                    with open(os.path.join(TABLES_PATH, csv_name), "w", encoding="utf-8-sig", newline="") as f:
                        writer = csv.writer(f)
                        for row in table:
                            writer.writerow([(cell or '').strip() for cell in row])
    basename = os.path.splitext(os.path.basename(pdf_path))[0]
    with open(os.path.join(PREVIEW_PATH, f"{basename}.txt"), "w", encoding="utf-8") as f:
        for obj in texts:
            f.write(f"[p{obj['page']}][{obj['type']}]\n{obj['content']}\n\n")
    with open(os.path.join(TABLES_PATH, f"{basename}.tables.json"), "w", encoding="utf-8") as f:
        json.dump(tables_data, f, ensure_ascii=False, indent=2)
    return texts, False

def extract_key_sections(text_objs, keywords):
    results = []
    for idx, obj in enumerate(text_objs):
        if any(kw in obj['content'] for kw in keywords):
            merged = obj['content']
            if obj['type'] == "text":
                for offset in range(1, 4):
                    if idx + offset < len(text_objs) and text_objs[idx + offset]['page'] == obj['page']:
                        nxt = text_objs[idx + offset]['content']
                        if re.search(r"[0-9,\.]+\s*(å„„|å„„å…ƒ|ä»Ÿå…ƒ|åƒå…ƒ|ç™¾è¬å…ƒ|è¬å…ƒ|å…ƒ)", nxt):
                            merged += "\n" + nxt
                        else:
                            break
            if obj['type'] == "text" and idx > 0 and text_objs[idx-1]['page'] == obj['page']:
                prev = text_objs[idx-1]['content']
                if len(prev) < 30 and re.search(r'(ç‡Ÿæ¥­|æ”¶å…¥|åˆè¨ˆ|å°è¨ˆ|ç¸½é¡|å–®ä½|æ˜ç´°)', prev):
                    merged = prev + "\n" + merged
            results.append({
                "content": merged,
                "page": obj['page'],
                "type": obj['type'],
            })
        if obj['type'] == "table":
            merged_table = obj['content']
            if idx > 0 and text_objs[idx-1]['page'] == obj['page']:
                prev = text_objs[idx-1]['content']
                if len(prev) < 30 and re.search(r'(ç‡Ÿæ¥­æ”¶å…¥|æ”¶å…¥æ˜ç´°|åˆè¨ˆ|å°è¨ˆ|é …ç›®|å–®ä½)', prev):
                    merged_table = prev + "\n" + obj['content']
            if any(kw in merged_table for kw in keywords):
                results.append({
                    "content": merged_table,
                    "page": obj['page'],
                    "type": obj['type'],
                })
    return results

def build_vector_db(all_objs, company, safe_folder):
    logging.info(f"å…¬å¸åŸå§‹å: {company} | è³‡æ–™å¤¾å®‰å…¨å: {safe_folder}")
    splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)
    docs = []
    for obj in all_objs:
        chunks = splitter.split_text(obj['content'])
        for split in chunks:
            docs.append(Document(
                page_content=split,
                metadata={
                    "company": company,
                    "page": obj['page'],
                    "type": obj['type'],
                }
            ))
    embedding = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2", model_kwargs={'device': 'cpu'})
    company_dir = os.path.join(VECTORSTORE_DIR, safe_folder)
    os.makedirs(company_dir, exist_ok=True)
    company_vector_path = os.path.join(company_dir, 'db_faiss')
    os.makedirs(company_vector_path, exist_ok=True)

    if not os.path.exists(company_vector_path):
        logging.error(f"âŒ å»ºç«‹è³‡æ–™å¤¾å¤±æ•—: {company_vector_path}")
        raise Exception(f"è³‡æ–™å¤¾ä¸å­˜åœ¨: {company_vector_path}")

    db = FAISS.from_documents(docs, embedding)
    db.save_local(company_vector_path)

    with open(os.path.join(CHUNKS_PATH, f"{safe_folder}_chunks.json"), "w", encoding="utf-8") as f:
        json.dump([{
            "content": d.page_content,
            **d.metadata
        } for d in docs], f, ensure_ascii=False, indent=2)
    logging.info(f"âœ… å‘é‡åº«+åˆ†æ®µmetadataå·²å­˜ï¼š{company_vector_path}")

def process_single_pdf(pdf_path, company):
    safe_folder = safe_folder_name(company)
    logging.info(f"{company}: {pdf_path}")
    keywords = load_keywords()
    texts, _ = extract_text_from_pdf(pdf_path)
    filtered = extract_key_sections(texts, keywords)
    build_vector_db(filtered, company, safe_folder)
    update_company_mapping(safe_folder, company)
    logging.info(f"ğŸ {company} å®Œæˆå‘é‡åº«èˆ‡è³‡æ–™å»ºç½®")

def batch_process_all(data_path=DATA_PATH):
    pdf_files = [f for f in os.listdir(data_path) if f.lower().endswith(".pdf")]
    logging.info(f"ğŸ” æ‰¹æ¬¡è™•ç† {len(pdf_files)} ä»½ PDF æª”æ¡ˆ")
    for f in pdf_files:
        company = os.path.splitext(os.path.basename(f))[0]
        try:
            process_single_pdf(os.path.join(data_path, f), company)
        except Exception as e:
            logging.error(f"âŒ [{company}] è™•ç†éŒ¯èª¤: {e}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--pdf", type=str, help="PDF æª”æ¡ˆè·¯å¾‘")
    parser.add_argument("--company", type=str, help="å…¬å¸åç¨±")
    parser.add_argument("--all", action='store_true', help="æ‰¹æ¬¡è™•ç† data/ å…§æ‰€æœ‰ PDF")
    args = parser.parse_args()
    t0 = datetime.now()

    if args.all:
        batch_process_all()
    elif args.pdf and args.company:
        process_single_pdf(args.pdf, args.company)
    else:
        print("è«‹è¼¸å…¥ --pdf æª”æ¡ˆè·¯å¾‘ åŠ --company å…¬å¸åç¨±ï¼Œæˆ– --all é€²è¡Œæ‰¹æ¬¡è™•ç†")
        sys.exit(0)
    logging.info(f"ğŸ å…¨éƒ¨è™•ç†å®Œæˆï¼Œç”¨æ™‚ {(datetime.now()-t0).total_seconds():.2f} ç§’")
