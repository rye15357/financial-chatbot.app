import streamlit as st
import os
from dotenv import load_dotenv
from huggingface_hub import login
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain_core.prompts import PromptTemplate
from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline
from fuzzywuzzy import process
from datetime import datetime
import openai
import torch
import re
import subprocess
from bs4 import BeautifulSoup
import requests
import pandas as pd
import time 
import hashlib
import sys
from lang_pack import LANG_PACK
import json
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import shutil

# ====== ä¸­æ–‡å­—å‹è¨­å®šï¼ˆæ”¯æ´æœ¬åœ°/é›²ç«¯ï¼‰======
FONT_PATH = os.path.join(os.path.dirname(__file__), "NotoSansTC-Regular.ttf")
if os.path.exists(FONT_PATH):
    prop = fm.FontProperties(fname=FONT_PATH)
    plt.rcParams['font.sans-serif'] = [prop.get_name(), 'sans-serif']
else:
    prop = None
    plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False

# === .env & Token åˆå§‹åŒ– ===
load_dotenv()
token = os.getenv("HUGGINGFACEHUB_API_TOKEN")
serper_api_key = os.getenv("SERPER_API_KEY")
openai_api_key = os.getenv("OPENAI_API_KEY")
newsapi_key = os.getenv("NEWSAPI_KEY")

def login_hf(token):
    try:
        login(token=token)
    except Exception as e:
        st.error(f"Hugging Face Token ç™»å…¥å¤±æ•—ï¼š{e}")
        st.stop()

if not token:
    st.error("è«‹è¨­å®š HUGGINGFACEHUB_API_TOKEN")
    st.stop()
if not openai_api_key:
    st.error("è«‹è¨­å®š OPENAI_API_KEY")
    st.stop()

# åªåœ¨ Session ç¬¬ä¸€æ¬¡ login
if "hf_logged_in" not in st.session_state:
    login_hf(token)
    st.session_state["hf_logged_in"] = True

openai.api_key = openai_api_key

# è·¯å¾‘åˆå§‹åŒ–
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
VECTOR_DIR = os.path.join(BASE_DIR, "vectorstore")
os.makedirs(VECTOR_DIR, exist_ok=True)
DATA_DIR = os.path.join(BASE_DIR, "data")
os.makedirs(DATA_DIR, exist_ok=True)

MAPPING_PATH = os.path.join(BASE_DIR, "company_mapping.json")

def load_company_mapping():
    if os.path.exists(MAPPING_PATH):
        with open(MAPPING_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    return {}

def save_company_mapping(mapping):
    with open(MAPPING_PATH, "w", encoding="utf-8") as f:
        json.dump(mapping, f, ensure_ascii=False, indent=2)

company_mapping = load_company_mapping()  # { safe_folder: ä¸­æ–‡å }

# ==== èŠå¤©ç´€éŒ„ç›¸é—œ ====

def ask_openai(prompt, model="gpt-4o", temperature=0.7, lang="ç¹é«”ä¸­æ–‡", history=None):
    # å‹•æ…‹å¤šæ®µè½åˆ†çµ„ system prompt
    if lang == "ç¹é«”ä¸­æ–‡":
        system_prompt = (
            "ä½ æ˜¯ä¸€ä½é ‚å°–çš„è²¡ç¶“ç”¢æ¥­åˆ†æå¸«ï¼Œå°ˆç²¾å°ç£/å…¨çƒä¸Šå¸‚æ«ƒå…¬å¸ã€åŠå°é«”èˆ‡AIç”¢æ¥­ã€‚"
            "è«‹æ ¹æ“šç”¨æˆ¶å•é¡Œèˆ‡è¼¸å…¥è³‡æ–™ï¼Œ**è‡ªå‹•åˆ†æ®µè½ï¼ˆæ¯æ®µæœ‰æ˜ç¢ºå°æ¨™é¡Œ+æ¢åˆ—é‡é»ï¼‰**ï¼Œ"
            "å¸¸è¦‹å¦‚ï¼šè²¡å‹™æ•¸æ“šã€æ¥­å‹™äº®é»ã€è¶¨å‹¢ã€æŒ‘æˆ°ã€å±•æœ›ã€é¢¨éšªã€æŠ•è³‡è§€é»ã€ç”¢æ¥­èƒŒæ™¯...ï¼ˆä¾å…§å®¹è‡ªå‹•æ±ºå®šï¼‰"
            "æ¯æ®µè½æ¨™é¡Œå¯åŠ  emojiï¼Œå…§å®¹ç”¨æ¢åˆ—æ¸…æ¥šå‘ˆç¾ï¼Œç„¡è³‡æ–™æ™‚è«‹ç›´æ¥èªªæ˜ã€‚"
            "å›ç­”æ ¼å¼è«‹ç”¨ markdownï¼Œ**ä¸è¦æœ‰å¤šé¤˜å¯’æš„æˆ–è´…è©**ã€‚"
        )
    else:
        system_prompt = (
            "You are a top financial/industry analyst. For each user question and data, "
            "dynamically group your answer into multiple sections, each with a clear headline (optionally emoji) and bullet points. "
            "Section topics may include: Financials, Highlights, Trends, Outlook, Risk, Analyst View, Background, etc. (decide based on context)."
            "Output in markdown. No chit-chat. State 'No clear info' if unavailable."
        )
    messages = [{"role": "system", "content": system_prompt}]
    if history:
        messages.extend(history)
    messages.append({"role": "user", "content": prompt})
    client = openai.OpenAI(api_key=openai.api_key)
    completion = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=temperature,
        max_tokens=1200
    )
    return completion.choices[0].message.content.strip()



def get_cached_summary_path(company):
    return os.path.join(VECTOR_DIR, company, "summary.txt")

def save_summary_to_cache(company, summary):
    summary_path = get_cached_summary_path(company)
    with open(summary_path, "w", encoding="utf-8") as f:
        f.write(summary)

def load_summary_from_cache(company):
    summary_path = get_cached_summary_path(company)
    if os.path.exists(summary_path):
        with open(summary_path, "r", encoding="utf-8") as f:
            return f.read()
    return None

def get_chat_path(user_id, topic):
    user_dir = f"chats/{user_id}"
    os.makedirs(user_dir, exist_ok=True)
    return f"{user_dir}/{topic}.json"

def save_chat(messages, user_id, topic):
    path = get_chat_path(user_id, topic)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(messages, f, ensure_ascii=False, indent=2)

def load_chat(user_id, topic):
    path = get_chat_path(user_id, topic)
    if os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    return []

def list_topics(user_id):
    user_dir = f"chats/{user_id}"
    if not os.path.exists(user_dir):
        return []
    return [f[:-5] for f in os.listdir(user_dir) if f.endswith('.json')]

def list_users():
    if not os.path.exists("chats"):
        return []
    return [d for d in os.listdir("chats") if os.path.isdir(os.path.join("chats", d))]

# ==== LLM QA & åˆ†æ ====
qa_prompt_template_zh = """
ä½ æ˜¯ä¸€ä½å°ˆæ¥­è²¡å ± AI åŠ©ç†ã€‚è«‹æ ¹æ“šä¸‹åˆ—è²¡å ±å…§å®¹ï¼Œé‡å°å•é¡Œç”¨ã€Œä¸€å¥è‡ªç„¶ã€æ¸…æ¥šã€ç°¡çŸ­ã€çš„ç¹é«”ä¸­æ–‡å›ç­”ï¼Œä¸è¦è¤‡è£½åŸæ–‡ï¼Œä¸éœ€è¦ä¾†æºèˆ‡é ç¢¼ã€‚

ğŸ“Š è²¡å ±å…§å®¹ï¼ˆåƒè€ƒï¼‰ï¼š
{context}

ğŸ” å•é¡Œï¼š
{question}

è«‹ç›´æ¥èªªæ˜é—œéµæ•¸å­—æˆ–çµè«–ã€‚å¦‚æœè²¡å ±ä¸­æ‰¾ä¸åˆ°æ˜ç¢ºç­”æ¡ˆï¼Œè«‹å›è¦†ã€ŒæŸ¥ç„¡æ˜ç¢ºè³‡æ–™ã€ã€‚
"""
qa_prompt_template_en = """
You are a professional financial report AI assistant. Based on the following report content, answer the question with a concise and natural English sentence. Do not copy original text or cite pages.

ğŸ“Š Financial Content (for reference only):
{context}

ğŸ” Question:
{question}

Directly state the key figure or conclusion. If no clear answer, reply: "No clear information found."
"""
analysis_prompt_template_zh = """
ä½ æ˜¯ä¸€ä½è²¡ç¶“é¡§å•ï¼Œæ ¹æ“šä¸‹åˆ—å…§å®¹é€²è¡Œæƒ…ç·’èˆ‡å‰æ™¯åˆ†æã€‚è«‹ç”¨ä¸€å¥è©±ç¸½çµï¼Œä¸¦æ¨™ç¤ºï¼ˆæ¨‚è§€ï¼ä¸­æ€§ï¼ä¿å®ˆï¼‰ã€‚

ğŸ“œ è²¡å‹™èªªæ˜ï¼š
{text}
"""
analysis_prompt_template_en = """
You are a financial consultant. Analyze the sentiment and outlook of the following financial statement. Use one sentence and indicate (Optimistic/Neutral/Conservative).

ğŸ“œ Financial Description:
{text}
"""

@st.cache_resource
def load_llm():
    try:
        model_id = "Qwen/Qwen1.5-1.8B-Chat"
        tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
        model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True).to("cuda" if torch.cuda.is_available() else "cpu")
        pipe = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            max_new_tokens=512,
            do_sample=True,
            temperature=0.7,
            top_k=50,
            top_p=0.95,
            repetition_penalty=1.1,
            pad_token_id=tokenizer.eos_token_id or tokenizer.pad_token_id or tokenizer.unk_token_id,
            device=0 if torch.cuda.is_available() else -1
        )
        return HuggingFacePipeline(pipeline=pipe)
    except Exception as e:
        st.error(f"æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
        return None

@st.cache_resource
def get_qa_bot(DB_FAISS_PATH, lang):
    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2', model_kwargs={'device': 'cpu'})
    try:
        db = FAISS.load_local(DB_FAISS_PATH, embeddings, allow_dangerous_deserialization=True)
    except Exception as e:
        st.error(f"å‘é‡è³‡æ–™åº«è¼‰å…¥å¤±æ•—: {e}")
        return None
    llm = load_llm()
    if not llm:
        return None
    if lang == "ç¹é«”ä¸­æ–‡":
        prompt = PromptTemplate(template=qa_prompt_template_zh, input_variables=['context', 'question'])
    else:
        prompt = PromptTemplate(template=qa_prompt_template_en, input_variables=['context', 'question'])
    retriever = db.as_retriever(search_type="mmr", search_kwargs={"k": 2})
    return RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        chain_type_kwargs={"prompt": prompt}
    )

@st.cache_resource
def get_analysis_pipeline(lang):
    llm = load_llm()
    if lang == "ç¹é«”ä¸­æ–‡":
        analysis_prompt = PromptTemplate(template=analysis_prompt_template_zh, input_variables=['text'])
    else:
        analysis_prompt = PromptTemplate(template=analysis_prompt_template_en, input_variables=['text'])
    return (analysis_prompt | llm)


def extract_twd_amount(text, query=""):
    keywords = ["ç‡Ÿæ¥­æ”¶å…¥", "æ”¶å…¥", "revenue", "sales", "income", "total"]
    try:
        if any(k in query.lower() for k in keywords) or any(k in text for k in keywords):
            match = re.search(r"(ç‡Ÿæ¥­æ”¶å…¥|æ”¶å…¥|revenue|sales|income)[^0-9]{0,10}([0-9,\.]+)\s*(å„„|å„„å…ƒ|ä»Ÿå…ƒ|åƒå…ƒ|ç™¾è¬å…ƒ|è¬å…ƒ|å…ƒ|million|billion)?", text, re.IGNORECASE)
            if match:
                num = match.group(2)
                unit = match.group(3) or ""
                return f"{num}{unit}"
        all_matches = re.findall(r"([0-9,\.]+)\s*(å„„|å„„å…ƒ|ä»Ÿå…ƒ|åƒå…ƒ|ç™¾è¬å…ƒ|è¬å…ƒ|å…ƒ)", text)
        if all_matches:
            biggest = max(all_matches, key=lambda x: float(x[0].replace(',', '')))
            return f"{biggest[0]}{biggest[1]}"
    except Exception:
        pass
    return None

def extract_eps_amount(text, query=""):
    # æ”¯æ´EPSå•æ³•
    if "eps" in query.lower() or "æ¯è‚¡ç›ˆé¤˜" in query or "EPS" in text:
        match = re.search(r"(EPS|æ¯è‚¡ç›ˆé¤˜)[^\d\-\.]{0,10}([\-]?[0-9]+(?:\.[0-9]+)?)", text, re.IGNORECASE)
        if match:
            return match.group(2)
    return None

def extract_net_income(text, query=""):
    if "æ·¨åˆ©" in query or "net income" in query.lower() or "æ·¨åˆ©" in text:
        match = re.search(r"(æ·¨åˆ©|net income)[^0-9\-]{0,10}([\-]?[0-9,\.]+)\s*(å„„|ç™¾è¬|è¬å…ƒ|å…ƒ)?", text, re.IGNORECASE)
        if match:
            return f"{match.group(2)}{match.group(3) or ''}"
    return None

def extract_gross_margin(text, query=""):
    if "æ¯›åˆ©ç‡" in query or "gross margin" in query.lower() or "æ¯›åˆ©ç‡" in text:
        match = re.search(r"(æ¯›åˆ©ç‡|gross margin)[^\d\-\.]{0,10}([\-]?[0-9\.]+)\s*%", text, re.IGNORECASE)
        if match:
            return f"{match.group(2)}%"
    return None

def extract_amount_by_type(text, query=""):
    # å°‹æ‰¾æ‰€æœ‰æœ‰å–®ä½çš„æ•¸å­—ï¼Œé¿å…å¹´ä»½
    matches = re.findall(r"([0-9,\.]+)\s*(å„„|å„„å…ƒ|ä»Ÿå…ƒ|åƒå…ƒ|ç™¾è¬|ç™¾è¬å…ƒ|è¬å…ƒ|å…ƒ|million|billion|%)", text)
    results = []
    for num, unit in matches:
        try:
            val = float(num.replace(",", ""))
            # å¹´ä»½ç¯„åœé€šå¸¸ä¸æœƒæœ‰å–®ä½ï¼Œä½†é‚„æ˜¯ä¿å®ˆæ’é™¤1900~2100
            if 1900 <= val <= 2100:
                continue
            results.append(f"{num}{unit}")
        except:
            continue
    if results:
        return results[0]
    return None


def safe_folder_name(name):
    ascii_name = re.sub(r"[^\w]", "_", name)
    ascii_name = re.sub(r"[^a-zA-Z0-9_]", "_", ascii_name)
    hash_part = hashlib.md5(name.encode("utf-8")).hexdigest()[:8]
    return f"{ascii_name.lower()}_{hash_part}"



def get_company_year_data(companies, indicator, years):
    """
    æŸ¥è©¢å¤šå®¶å…¬å¸å¤šå¹´åº¦å–®ä¸€æŒ‡æ¨™ï¼ˆå¦‚ç‡Ÿæ”¶/EPSï¼‰è³‡æ–™ä¾†æºï¼šWeb > OpenAI > PDF
    åƒæ•¸ï¼š
        companies: ["å°ç©é›»", "é´»æµ·"]
        indicator: "ç‡Ÿæ”¶" æˆ– "Revenue"
        years: [2021,2022,2023]
    å›å‚³ï¼šDataFrame æ¬„ä½ ["å…¬å¸", "å¹´åº¦", indicator, "ä¾†æº"]
    """
    rows = []
    search_map = {
        "ç‡Ÿæ”¶": "ç‡Ÿæ”¶", "Revenue": "ç‡Ÿæ”¶",
        "æ·¨åˆ©": "æ·¨åˆ©", "Net Income": "æ·¨åˆ©",
        "EPS": "EPS",
        "æ¯›åˆ©ç‡": "æ¯›åˆ©ç‡", "Gross Margin": "æ¯›åˆ©ç‡"
    }
    for company in companies:
        for year in years:
            # 1. æŸ¥ Web
            query = f"{company} {year}å¹´ {search_map[indicator]}"
            web_result = web_search(query)
            match = re.search(r'([0-9,\.]+)\s*(å„„|ä»Ÿå…ƒ|åƒå…ƒ|ç™¾è¬|è¬å…ƒ|å…ƒ|%)', web_result)
            if match:
                value, unit = match.groups()
                val = float(value.replace(",", ""))
                if "%" in unit:
                    pass
                else:
                    multiplier = {
                        "å„„": 1e8, "ä»Ÿå…ƒ": 1e3, "åƒå…ƒ": 1e3,
                        "ç™¾è¬": 1e6, "è¬å…ƒ": 1e4, "å…ƒ": 1
                    }.get(unit, 1)
                    val = val * multiplier
                rows.append({"å…¬å¸": company, "å¹´åº¦": year, indicator: val, "ä¾†æº": "ç¶²è·¯"})
                continue

            # 2. æŸ¥ GPT-4o
            prompt = f"è«‹åˆ—å‡º{company} {year}å¹´{indicator}ï¼ˆå–®ä½ï¼šå„„å…ƒæˆ–%ï¼‰ï¼Œåªçµ¦æ•¸å­—ï¼Œä¸è¦èªªæ˜ï¼Œå›å‚³æ ¼å¼ï¼š{year},{indicator}=?"
            gpt_result = ask_openai(prompt, model="gpt-4o")
            match2 = re.search(r'([0-9,\.]+)\s*(å„„|ä»Ÿå…ƒ|åƒå…ƒ|ç™¾è¬|è¬å…ƒ|å…ƒ|%)', gpt_result)
            if match2:
                value, unit = match2.groups()
                val = float(value.replace(",", ""))
                if "%" in unit:
                    pass
                else:
                    multiplier = {
                        "å„„": 1e8, "ä»Ÿå…ƒ": 1e3, "åƒå…ƒ": 1e3,
                        "ç™¾è¬": 1e6, "è¬å…ƒ": 1e4, "å…ƒ": 1
                    }.get(unit, 1)
                    val = val * multiplier
                rows.append({"å…¬å¸": company, "å¹´åº¦": year, indicator: val, "ä¾†æº": "GPT-4o"})
                continue

            # 3. æŸ¥ PDFï¼ˆæœ¬åœ°ï¼‰
            safe_names = [k for k, v in (company_mapping or {}).items() if v == company]
            if safe_names:
                safe_name = safe_names[0]
                DB_FAISS_PATH = os.path.join(VECTOR_DIR, safe_name, "db_faiss")
                if os.path.exists(DB_FAISS_PATH):
                    qa = get_qa_bot(DB_FAISS_PATH, "ç¹é«”ä¸­æ–‡")
                    if qa:
                        pdf_result = qa.invoke({"query": f"{year}å¹´{indicator}å¤šå°‘"})
                        pdf_val = extract_amount_by_type(str(pdf_result), f"{year}å¹´{indicator}")
                        if pdf_val:
                            try:
                                val = float(re.sub(r"[^\d\.]", "", pdf_val))
                                rows.append({"å…¬å¸": company, "å¹´åº¦": year, indicator: val, "ä¾†æº": "PDF"})
                            except:
                                pass
    df = pd.DataFrame(rows)
    return df

def extract_company_name(user_input, company_name_list):
    # ç›´æ¥å‘½ä¸­
    for name in sorted(company_name_list, key=len, reverse=True):
        if name in user_input:
            return name
    # è‚¡ç¥¨ä»£ç¢¼
    code_to_name = {code: name for code, name in company_mapping.items()}
    stock_code_match = re.search(r"\d{4}", user_input)
    if stock_code_match and stock_code_match.group(0) in code_to_name:
        return code_to_name[stock_code_match.group(0)]
    # Fuzzy match
    result = process.extractOne(user_input, company_name_list, score_cutoff=80)
    if result:
        match, score = result
        return match
    return None

def goodinfo_web_search(query, max_len=500):
    """
    Goodinfo! è²¡å ±æ•¸æ“šï¼ˆåƒ…æ”¯æŒè‚¡ç¥¨ä»£ç¢¼ï¼Œå¦‚2330ï¼‰
    """
    try:
        stock_id = re.search(r"\d{4}", query)
        if not stock_id:
            return "è«‹è¼¸å…¥è‚¡ç¥¨ä»£ç¢¼ï¼ˆå¦‚2330ï¼‰"
        code = stock_id.group(0)
        url = f"https://goodinfo.tw/tw/StockFinDetail.asp?RPT_CAT=XX_M_QUAR_ACC&STOCK_ID={code}"
        resp = requests.get(url, timeout=8, headers={"user-agent":"Mozilla/5.0"})
        resp.encoding = "utf-8"
        soup = BeautifulSoup(resp.text, "html.parser")
        table = soup.find("table", class_="b1 p4_2 r0_10 row_mouse_over")
        if not table:
            return "æŸ¥ç„¡è²¡å ±"
        text = table.get_text(separator="\n", strip=True)
        return f"ã€Goodinfo!ã€‘\n{text[:max_len]}\n{url}"
    except Exception as e:
        return f"Goodinfo! æŸ¥è©¢å¤±æ•—: {e}"


def cnyes_web_search(query, max_len=600):
    """
    é‰…äº¨ç¶²é—œéµå­—çˆ¬èŸ²ï¼Œåªå›å‚³ç¬¬ä¸€ç­†æ–°èæ¨™é¡Œï¼‹æ‘˜è¦ï¼‹é€£çµ
    """
    try:
        url = f"https://search.cnyes.com/news/newslist?q={query}&t=keyword"
        resp = requests.get(url, timeout=8, headers={"user-agent": "Mozilla/5.0"})
        resp.encoding = "utf-8"
        soup = BeautifulSoup(resp.text, "html.parser")
        item = soup.select_one(".tabList .newsItem")
        if not item:
            return "æŸ¥ç„¡æ–°è"
        title_tag = item.select_one(".newsItem__title")
        desc_tag = item.select_one(".newsItem__summury")
        link_tag = item.select_one("a")
        title = title_tag.text.strip() if title_tag else ""
        desc = desc_tag.text.strip() if desc_tag else ""
        link = "https://news.cnyes.com" + link_tag['href'] if link_tag else ""
        return f"ã€é‰…äº¨ç¶²ã€‘\n{title}\n{desc[:max_len]}\n{link}"
    except Exception as e:
        return f"é‰…äº¨ç¶²æŸ¥è©¢å¤±æ•—: {e}"

def twse_api_search(query):
    """
    è­‰äº¤æ‰€å…¬é–‹è³‡è¨Šè§€æ¸¬ç«™ OpenAPI
    å®˜æ–¹èªªæ˜ï¼šhttps://openapi.twse.com.tw/
    """
    try:
        stock_id = re.search(r"\d{4}", query)
        if not stock_id:
            return "è«‹è¼¸å…¥è‚¡ç¥¨ä»£ç¢¼ï¼ˆå¦‚2330ï¼‰"
        code = stock_id.group(0)
        url = f"https://openapi.twse.com.tw/v1/opendata/t187ap03_L"
        resp = requests.get(url, timeout=10)
        data = resp.json()
        for item in data:
            if item.get("å…¬å¸ä»£è™Ÿ") == code:
                summary = "\n".join([f"{k}: {v}" for k, v in item.items()])
                return f"ã€è­‰äº¤æ‰€ APIã€‘\n{summary}"
        return "æŸ¥ç„¡å…¬é–‹è³‡è¨Š"
    except Exception as e:
        return f"è­‰äº¤æ‰€ API æŸ¥è©¢å¤±æ•—: {e}"


def check_company_status_tw(company_name):
    try:
        url = f"https://www.tw-inc.com/company/search?q={company_name}"
        headers = {"User-Agent": "Mozilla/5.0"}
        r = requests.get(url, headers=headers)
        soup = BeautifulSoup(r.text, "html.parser")
        result_block = soup.select_one(".company-list .item")
        if not result_block:
            return "æŸ¥ç„¡è³‡æ–™", url
        status_text = result_block.text
        if "è§£æ•£" in status_text or "æ’¤éŠ·" in status_text or "å»¢æ­¢" in status_text:
            return "è§£æ•£ï¼å·²çµæŸç‡Ÿæ¥­", url
        elif "æ ¸å‡†è¨­ç«‹" in status_text or "ç‡Ÿæ¥­ä¸­" in status_text:
            return "ç‡Ÿæ¥­ä¸­", url
        else:
            return "æœªçŸ¥ç‹€æ…‹", url
    except Exception:
        return "æŸ¥ç„¡è³‡æ–™", ""



def newsapi_search(query, api_key=None, max_len=500):
    """
    NewsAPI åœ‹éš›æ–°èèšåˆæŸ¥è©¢ï¼ˆéœ€è¨»å†Šå–å¾— API KEYï¼‰
    """
    if not api_key:
        return "è«‹è¨­å®š NewsAPI API KEY"
    try:
        url = "https://newsapi.org/v2/everything"
        params = {"q": query, "language": "zh", "apiKey": api_key, "pageSize": 1}
        resp = requests.get(url, params=params, timeout=8)
        data = resp.json()
        if data.get("status") == "ok" and data.get("totalResults", 0) > 0:
            article = data["articles"][0]
            title = article["title"]
            desc = article["description"] or ""
            link = article["url"]
            return f"ã€NewsAPIã€‘\n{title}\n{desc[:max_len]}\n{link}"
        return "æŸ¥ç„¡æ–°è"
    except Exception as e:
        return f"NewsAPI æŸ¥è©¢å¤±æ•—: {e}"


def synthesize_answers(query, search_results, lang="ç¹é«”ä¸­æ–‡"):
    prompt = (
        "è«‹æ ¹æ“šä»¥ä¸‹å…©å€‹ä¾†æºè³‡æ–™ï¼Œå¹«æˆ‘æ¢åˆ—å½™æ•´é‡é»ä¸¦ç¸½çµçµè«–ï¼ˆä¸ç”¨é™„ç¶²å€ï¼‰ï¼š\n\n"
        if lang == "ç¹é«”ä¸­æ–‡" else
        "Based on the following two sources, summarize key points and provide a conclusion (no links):\n\n"
    )
    for src, content in search_results:
        prompt += f"ã€{src}ã€‘\n{content}\n\n"
    prompt += "è«‹å¼·èª¿é—œéµæ•¸æ“šèˆ‡ä¸åŒè§€é»ï¼Œæœ€å¾Œç”¨ä½ è‡ªå·±çš„å°ˆæ¥­èªæ°£ç¸½çµã€‚"
    return ask_openai(prompt, lang=lang)


def extract_status_from_text(text):
    # è§£æ•£é—œéµå­—
    if any(k in text for k in ["è§£æ•£", "æ­‡æ¥­", "æ’¤éŠ·", "çµæŸç‡Ÿæ¥­", "å»¢æ­¢", "å·²ä¸å­˜åœ¨", "å·²ä¸‹å¸‚"]):
        return "å·²è§£æ•£"
    # ç‡Ÿæ¥­ä¸­é—œéµå­—
    if any(k in text for k in ["ç‡Ÿæ¥­ä¸­", "æ ¸å‡†è¨­ç«‹", "ç¾å­˜", "å­˜çºŒ", "ä¸Šå¸‚", "ä¸Šæ«ƒ", "ç™»è¨˜è¨­ç«‹"]):
        return "ç‡Ÿæ¥­ä¸­"
    # å°ç£å…¬å¸ç¶²æœ‰å…¬å¸è³‡è¨Š
    if "çµ±ç·¨" in text and ("æœ‰é™å…¬å¸" in text or "è‚¡ä»½æœ‰é™å…¬å¸" in text or "å…¬å¸" in text):
        return "ç‡Ÿæ¥­ä¸­"
    # æœ‰å…¬å¸åœ°å€ã€è² è²¬äºº
    if any(k in text for k in ["è² è²¬äºº", "åœ°å€", "è¨­ç«‹"]):
        return "ç‡Ÿæ¥­ä¸­"
    return "æŸ¥ç„¡"


def get_latest_company_status_from_sources(news_results):
    # éæ­·å¤šå€‹ä¾†æºå›å‚³çš„å…§å®¹ï¼Œè‡ªå‹•åˆ¤æ–·å…¬å¸ç‹€æ…‹
    status_list = []
    for src, content in news_results:
        status = extract_status_from_text(content)
        status_list.append(status)
    # å„ªå…ˆå·²è§£æ•£ï¼Œå…¶æ¬¡ç‡Ÿæ¥­ä¸­
    if "å·²è§£æ•£" in status_list:
        return "å·²è§£æ•£"
    if "ç‡Ÿæ¥­ä¸­" in status_list:
        return "ç‡Ÿæ¥­ä¸­"
    return "æŸ¥ç„¡"


def integrated_ai_summary(user_input, DB_FAISS_PATH, multi_lang):
    # 1. å¤šä¾†æºæœå°‹
    news_results = multi_source_search(user_input)
    status_str = get_latest_company_status_from_sources(news_results)
    status_bar = ""
    if status_str == "å·²è§£æ•£":
        status_bar = "ğŸ”´ **å…¬å¸ç›®å‰ç‹€æ…‹ï¼šå·²è§£æ•£ï¼çµæŸç‡Ÿæ¥­**\n\n"
    elif status_str == "ç‡Ÿæ¥­ä¸­":
        status_bar = "ğŸŸ¢ **å…¬å¸ç›®å‰ç‹€æ…‹ï¼šç‡Ÿæ¥­ä¸­**\n\n"


    # ========== å°ç£å…¬å¸ç¶²æŸ¥ç„¡è³‡æ–™æé†’ ==========
    tw_company_result = ""
    for src, content in news_results:
        if src == "å°ç£å…¬å¸ç¶²":
            tw_company_result = content
            break
    # æ“´å……åˆ¤æ–·æ¢ä»¶
    no_company_info = (
        "æŸ¥ç„¡å…¬å¸è³‡æ–™" in tw_company_result or
        "æŸ¥ç„¡è³‡æ–™" in tw_company_result or
        "å…¬å¸ä¸å­˜åœ¨" in tw_company_result or
        "æ‰¾ä¸åˆ°è©²å…¬å¸" in tw_company_result
    )
    if no_company_info:
        missing_company_msg = (
            "âš ï¸ æ‰¾ä¸åˆ°è©²å…¬å¸ç‡Ÿé‹è³‡è¨Šï¼Œå¯èƒ½å·²æ­‡æ¥­ã€è§£æ•£ã€æ’¤éŠ·ã€æ”¹åæˆ–è³‡æ–™å·²ä¸‹æ¶ã€‚è«‹ç¢ºèªå…¬å¸åç¨±æ­£ç¢ºã€‚\n\n"
            if multi_lang == "ç¹é«”ä¸­æ–‡"
            else "âš ï¸ Company information not found; it may be dissolved, revoked, renamed, or removed. Please check the company name.\n\n"
        )
    else:
        missing_company_msg = ""

    context_text = ""
    for src, content in news_results:
        context_text += f"[{src}] {content}\n"

    # 2. PDF æœ¬åœ°è²¡å ±
    pdf_summary = ""
    if DB_FAISS_PATH and os.path.exists(DB_FAISS_PATH):
        qa = get_qa_bot(DB_FAISS_PATH, multi_lang)
        if qa:
            result = qa.invoke({"query": user_input})
            if isinstance(result, dict):
                pdf_summary = result.get("result") or result.get("output_text") or ""
            else:
                pdf_summary = result if isinstance(result, str) else str(result)
    if pdf_summary:
        context_text += f"[PDF è²¡å ±] {pdf_summary}\n"

    # 3. ä¸Ÿçµ¦ AIï¼Œè‡ªå‹•åˆ†æ®µ + å°æ¨™ + æ¢åˆ—ï¼Œç„¡è³‡æ–™èªªæ˜
    prompt = (
        f"{missing_company_msg}" + 
        (
            "è«‹æ ¹æ“šæ‰€æœ‰ä»¥ä¸‹è³‡æ–™ï¼Œè‡ªå‹•åˆ†æ®µå›ç­”ï¼ˆæ¯æ®µæœ‰æ˜ç¢ºå°æ¨™é¡Œ+æ¢åˆ—é‡é»ï¼‰ï¼Œ"
            "æ®µè½æ•¸èˆ‡æ¨™é¡Œå…§å®¹è«‹æ ¹æ“šå•é¡Œå‹•æ…‹æ±ºå®šï¼Œå¯åŒ…å«ï¼šæ•¸æ“šé‡é»ã€äº®é»ã€è¶¨å‹¢ã€é¢¨éšªã€æ¯”è¼ƒã€å±•æœ›ã€ç¸½çµç­‰ï¼Œ"
            "ç„¡è³‡æ–™æ™‚è«‹èªªæ˜ï¼Œå…¨éƒ¨ç”¨ markdown æ ¼å¼ï¼Œä¸è¦æœ‰å¯’æš„ã€‚"
            if multi_lang == "ç¹é«”ä¸­æ–‡" else
            "Based on ALL the following sources, auto-group answer into multiple sections (each with a headline & bullet points). "
            "Section count and topics depend on the question, can include: data, highlights, trends, risks, comparison, outlook, summary, etc. "
            "Say 'No clear data' if nothing found. Output markdown only."
        )
    )
    ai_ans = ask_openai(
        f"{prompt}\n\nç”¨æˆ¶å•é¡Œï¼š{user_input}\n\n{context_text}",
        lang=multi_lang
    )
    return status_bar + ai_ans



def multi_source_search(query):
    results = []
    # 1. Google
    results.append(("Google", web_search(query)))
    # 2. Yahooè²¡ç¶“
    results.append(("Yahooè²¡ç¶“", yahoo_finance_web_search(query)))
    # 3. å°ç£å…¬å¸ç¶²
    results.append(("å°ç£å…¬å¸ç¶²", taiwan_company_web_search(query)))
    # 4. å…¬é–‹è³‡è¨Šè§€æ¸¬ç«™ (MOPS)
    results.append(("å…¬é–‹è³‡è¨Šè§€æ¸¬ç«™", mops_web_search(query)))
    # 5. é‰…äº¨ç¶²
    results.append(("é‰…äº¨ç¶²", cnyes_web_search(query)))
    # 6. Goodinfo!
    results.append(("Goodinfo!", goodinfo_web_search(query)))
    # 7. è­‰äº¤æ‰€ API
    results.append(("è­‰äº¤æ‰€ API", twse_api_search(query)))
    # 8. NewsAPI 
    newsapi_key = os.getenv("NEWSAPI_KEY")
    if newsapi_key:
        results.append(("NewsAPI", newsapi_search(query, api_key=newsapi_key)))
    else:
        results.append(("NewsAPI", "æœªè¨­å®š API KEY"))
    return results


def yahoo_finance_web_search(query, max_len=600):
    """
    æŸ¥è©¢ Yahoo è²¡ç¶“å°ç£ï¼ˆtw.stock.yahoo.comï¼‰ï¼ŒæŠ“å…¬å¸ç°¡æ˜“è²¡å ±ï¼ˆç‡Ÿæ”¶ã€EPSã€æ·¨åˆ©ç­‰ï¼‰ã€‚
    å‚³å…¥å…¬å¸åç¨±æˆ–è‚¡ç¥¨ä»£ç¢¼çš†å¯ã€‚
    """
    try:
        # å˜—è©¦å¾ query æŠ“å‡ºå…¬å¸è‚¡ç¥¨ä»£ç¢¼ï¼ˆå¦‚2330ï¼‰
        stock_id = re.search(r"\d{4}", query)
        if stock_id:
            stock_code = stock_id.group(0)
        else:
            # å¦‚æœåªçµ¦å…¬å¸åè¦è½‰è‚¡ç¥¨ä»£ç¢¼ï¼Œå¯ç”¨å­—å…¸ mapping æˆ–ç”¨å…¬é–‹è³‡è¨Šè§€æ¸¬ç«™/å°ç£å…¬å¸ç¶²è£œæŸ¥
            return "è«‹è¼¸å…¥è‚¡ç¥¨ä»£ç¢¼æˆ–å…¬å¸åç¨±"

        url = f"https://tw.stock.yahoo.com/quote/{stock_code}/financial"
        resp = requests.get(url, timeout=8, headers={"user-agent":"Mozilla/5.0"})
        resp.encoding = "utf-8"
        soup = BeautifulSoup(resp.text, "html.parser")
        # æŠ“è²¡å ±è¡¨æ ¼
        table = soup.find("table")
        if not table:
            return "æŸ¥ç„¡è²¡å ±è³‡æ–™"
        text = table.get_text(separator="\n", strip=True)
        return f"ã€Yahooè²¡ç¶“ã€‘\n{text[:max_len]}\n{url}"
    except Exception as e:
        return f"Yahooè²¡ç¶“æŸ¥è©¢å¤±æ•—: {e}"


def mops_web_search(query, max_len=600):
    """
    ç”¨å…¬é–‹è³‡è¨Šè§€æ¸¬ç«™ï¼ˆMOPSï¼‰æŸ¥å…¬å¸åŸºæœ¬è³‡æ–™ã€é‡å¤§è¨Šæ¯
    """
    try:
        # é€™è£¡èˆ‰ä¾‹ç”¨å…¬é–‹è³‡è¨Šè§€æ¸¬ç«™å…¬å¸æŸ¥è©¢é é¢
        search_url = f"https://mops.twse.com.tw/mops/web/t05st01"
        params = {
            "TYPEK": "all",
            "firstin": "true",
            "co_id": "",      
            "keyword": query, 
        }
        # ç›´æ¥æŸ¥é—œéµå­—å…¶å¯¦æœ‰é™åˆ¶ï¼Œå»ºè­°å¯ç”¨å°ç£è­‰åˆ¸å…¬å¸ä»£ç¢¼å°ç…§è¡¨è¼”åŠ©
        resp = requests.get(search_url, params=params, timeout=10)
        resp.encoding = "utf-8"
        soup = BeautifulSoup(resp.text, "html.parser")
        table = soup.find("table", {"class": "hasBorder"})
        if not table:
            return "æŸ¥ç„¡å…¬å¸è³‡æ–™"
        text = table.get_text(separator="\n", strip=True)
        return f"ã€å…¬é–‹è³‡è¨Šè§€æ¸¬ç«™ã€‘\n{text[:max_len]}\n{search_url}"
    except Exception as e:
        return f"å…¬é–‹è³‡è¨Šè§€æ¸¬ç«™æŸ¥è©¢å¤±æ•—: {e}"


def taiwan_company_web_search(query, max_len=500):
    """çˆ¬å–å°ç£å…¬å¸ç¶²ï¼ˆtwincn.comï¼‰é—œéµå…¬å¸è³‡è¨Š"""
    try:
        url = f"https://www.twincn.com/search?q={query}"
        resp = requests.get(url, timeout=8)
        resp.encoding = "utf-8"
        soup = BeautifulSoup(resp.text, "html.parser")
        link_tag = soup.select_one("div.r-list > a")
        if not link_tag:
            return "æŸ¥ç„¡å…¬å¸è³‡æ–™"
        company_url = "https://www.twincn.com" + link_tag['href']
        company_resp = requests.get(company_url, timeout=8)
        company_resp.encoding = "utf-8"
        company_soup = BeautifulSoup(company_resp.text, "html.parser")
        summary_div = company_soup.select_one("div.r-info")
        summary_text = summary_div.get_text(separator="\n", strip=True) if summary_div else ""
        return f"ã€å°ç£å…¬å¸ç¶²ã€‘\n{summary_text[:max_len]}\n{company_url}"
    except Exception as e:
        return f"å°ç£å…¬å¸ç¶²æŸ¥è©¢å¤±æ•—: {e}"


def web_search(query, retry=2):
    url = "https://google.serper.dev/search"
    payload = {"q": query}
    headers = {
        "X-API-KEY": serper_api_key,
        "Content-Type": "application/json"
    }
    for i in range(retry+1):
        try:
            resp = requests.post(url, json=payload, headers=headers, timeout=8)
            data = resp.json()
            results = []
            all_snippets = ""
            for r in data.get("organic", [])[:3]:
                title = r.get("title")
                link = r.get("link")
                snippet = r.get("snippet", "")
                results.append(f"ã€{title}ã€‘\n{snippet}\n{link}")
                all_snippets += snippet + " "
            match = re.search(r'([0-9,\.]+)\s*(å„„|å„„å…ƒ|ä»Ÿå…ƒ|åƒè¬å…ƒ|ç™¾è¬å…ƒ|è¬å…ƒ|å…ƒ)', all_snippets)
            if match:
                amount = match.group(0)
                return f"\U0001f310 ä¾†æºï¼šç¶²è·¯\n\U0001f4b0 é‡‘é¡æ“·å–ï¼š**{amount}**\n\n" + "\n\n".join(results)
            return "\U0001f310 ä¾†æºï¼šç¶²è·¯\n" + "\n\n".join(results) if results else "âŒ ç„¡æ³•å¾ç¶²è·¯æŸ¥å¾—è³‡æ–™"
        except Exception as e:
            if i < retry:
                continue
            return f"âŒ ç¶²è·¯æŸ¥è©¢éŒ¯èª¤ï¼š{e}"

def parse_web_search_result(news_result, query=""):
    amount = extract_amount_by_type(news_result, query)
    news_list = []
    source_snippets = []
    news_pattern = r"ã€(.+?)ã€‘\n(.+?)\n(https?://[^\s]+)"
    for m in re.finditer(news_pattern, news_result):
        news_list.append({
            "title": m.group(1),
            "desc": m.group(2),
            "link": m.group(3)
        })
        source_snippets.append(m.group(2))
    return amount or "ï¼ˆæŸ¥ç„¡ï¼‰", news_list, source_snippets


def build_ai_reply(company, user_input, answer, amount, growth, news_list, source_snippets, T):
    bullets = []
    if answer:
        bullets.append(f"{company} {T['revenue']} {amount}, {T['yoy_growth']} {growth}.")
        bullets.append(T["ai_bullet1"])
        bullets.append(T["ai_bullet2"])
    else:
        bullets.append(T["no_data"])
    news_md = "\n".join([f"- [{n['title']}]({n['link']})ï¼š{n['desc']}" for n in news_list]) if news_list else T["no_news"]
    sources_md = "\n".join([f"> {s}" for s in source_snippets]) if source_snippets else ""
    reply = f"""

{T['ai_reply_title']}

---
{T['key_data_card']}
- **{T['company']}**ï¼š{company}
- **{T['query']}**ï¼š{user_input}
- **{T['revenue']}**ï¼š<span style="color:orange;font-weight:bold;">{amount}</span>
- **{T['yoy_growth']}**ï¼š{growth}

---

{T['ai_bullets']}
{chr(10).join([f"{i+1}. {b}" for i, b in enumerate(bullets)])}

---

{T['news_sources']}
{news_md}

---

{sources_md}
{T['also_ask']}

---
{T['ai_footer']}
"""
    return reply


# ==== å…¬å¸/å‘é‡/èªè¨€ ====
def get_companies_list():
    if not os.path.exists(VECTOR_DIR):
        return []
    return [d for d in os.listdir(VECTOR_DIR) if os.path.isdir(os.path.join(VECTOR_DIR, d))]

# ==== Streamlit ä»‹é¢ ====
st.set_page_config(page_title="è²¡å ±åŠ©ç†AI", page_icon=":robot_face:", layout="wide")
multi_lang = st.sidebar.selectbox(
    LANG_PACK["ç¹é«”ä¸­æ–‡"]["language_select"],
    list(LANG_PACK.keys()),
    key="lang_select"
)
if "lang_last" not in st.session_state or st.session_state["lang_last"] != multi_lang:
    st.session_state["lang_last"] = multi_lang
    st.rerun()
T = LANG_PACK[multi_lang]

# --- Sidebar: ç”¨æˆ¶/ä¸»é¡Œ ---
st.sidebar.markdown("## âš™ï¸ " + T["model_setting"])
model_options = ["Qwen1.8", "OpenAI GPT-4o"]
selected_model = st.sidebar.selectbox(T["model_select"], model_options, key="model_select")

st.sidebar.header(T["user_and_topic"])
users = list_users()
if not users:
    user_id = st.sidebar.text_input(T["user_input"], value="guest", key="user_id_new")
    if st.sidebar.button(T["add_user"]):
        if user_id:
            os.makedirs(f"chats/{user_id}", exist_ok=True)
            st.success(f"{T['add_user']}ï¼š{user_id}")
            st.rerun()
else:
    user_options = users + [T["add_user"]]
    user_id = st.sidebar.selectbox("ğŸ‘¤ " + T["user_label"], user_options, index=0, key="user_select")

    if user_id == T["add_user"]:
        new_user = st.sidebar.text_input(T["user_input"], key="new_user_id")
        if st.sidebar.button("âœ… " + T["add_user_btn"], key="add_user_btn"):
            if new_user and new_user not in users:
                os.makedirs(f"chats/{new_user}", exist_ok=True)
                st.success(f"âœ… {T['add_user']}ï¼š{new_user}")
                st.rerun()
            else:
                st.warning(T["user_input"])
    else:
        st.session_state["user_id"] = user_id

# ==== ğŸ“ ä¸»é¡Œé¸æ“‡ ====
all_add_topic_names = [v["add_topic"] for v in LANG_PACK.values()] + ["+ æ–°å¢ä¸»é¡Œ", "+ Add Topic"]
topics = list_topics(user_id)
topic_options = [t for t in topics if t not in all_add_topic_names]
topic_options.append(T["add_topic"])
topic = st.sidebar.selectbox(
    "ğŸ“ " + T["topic_label"],
    topic_options,
    index=0,
    key=f"topic_select_{user_id}"
)


if topic == T["add_topic"]:
    topic_new = st.sidebar.text_input(T["topic_input"], key="new_topic_name")
    if st.sidebar.button("âœ… " + T["confirm_add_topic"], key="add_topic_btn"):
        path = f"chats/{user_id}/{topic_new}.json"
        if not os.path.exists(path):
            with open(path, "w", encoding="utf-8") as f:
                json.dump([], f)
            st.success(f"âœ… å·²æ–°å¢ä¸»é¡Œï¼š{topic_new}")
            st.rerun()
        else:
            st.warning("âš ï¸ ä¸»é¡Œå·²å­˜åœ¨" if multi_lang == "ç¹é«”ä¸­æ–‡" else "âš ï¸ Topic already exists")
else:
    st.session_state["topic"] = topic

# åˆªé™¤ç”¨æˆ¶ï¼ˆå…©éšæ®µç¢ºèªï¼‰
delete_user_key = f"delete_user_confirm_{user_id}"
if user_id and user_id != T["add_user"]:
    if st.sidebar.button(f"{T['delete_user']} [{user_id}]", key=f"delete_user_btn_{user_id}"):
        st.session_state[delete_user_key] = True
    if st.session_state.get(delete_user_key, False):
        st.sidebar.warning("âš ï¸ æ­¤æ“ä½œä¸å¯å¾©åŸï¼Œè«‹å†æ¬¡é»æ“Šä¸‹æ–¹æŒ‰éˆ•ç¢ºèªï¼" if multi_lang=="ç¹é«”ä¸­æ–‡" else "âš ï¸ This cannot be undone, click again to confirm!")
        if st.sidebar.button("âš¡ï¸ ç¢ºèªæ°¸ä¹…åˆªé™¤" if multi_lang=="ç¹é«”ä¸­æ–‡" else "âš¡ï¸ Confirm Permanent Delete", key=f"delete_user_really_{user_id}"):
            user_dir = os.path.join("chats", user_id)
            try:
                shutil.rmtree(user_dir)
                st.success(f"{T['delete_user']}ï¼š{user_id}")
                st.session_state[delete_user_key] = False
                st.rerun()
            except Exception as e:
                st.error(f"åˆªé™¤ç”¨æˆ¶å¤±æ•—ï¼š{e}" if multi_lang=="ç¹é«”ä¸­æ–‡" else f"Delete user failed: {e}")


else:
    if st.sidebar.button("åˆ‡æ›ä¸»é¡Œ" if multi_lang=="ç¹é«”ä¸­æ–‡" else "Switch Topic"):
        st.session_state["messages"] = load_chat(user_id, topic)
        st.success(f"åˆ‡æ›åˆ°ç”¨æˆ¶ [{user_id}] ä¸»é¡Œ [{topic}]" if multi_lang=="ç¹é«”ä¸­æ–‡" else f"Switched to user [{user_id}], topic [{topic}]")
    if "messages" not in st.session_state:
        st.session_state["messages"] = load_chat(user_id, topic)

# åˆªé™¤ä¸»é¡Œï¼ˆå…©éšæ®µï¼‰

delete_topic_key = f"delete_topic_confirm_{topic}"
if topic != T["add_topic"]:  # æ’é™¤æ–°å¢ä¸»é¡Œé¸é …
    if st.sidebar.button(f"{T['delete_topic']} [{topic}]", key=f"delete_topic_btn_{topic}"):
        st.session_state[delete_topic_key] = True
    if st.session_state.get(delete_topic_key, False):
        st.sidebar.warning("âš ï¸ æ­¤æ“ä½œä¸å¯å¾©åŸï¼Œè«‹å†æ¬¡é»æ“Šä¸‹æ–¹æŒ‰éˆ•ç¢ºèªï¼" if multi_lang=="ç¹é«”ä¸­æ–‡" else "âš ï¸ This cannot be undone, click again to confirm!")
        if st.sidebar.button("âš¡ï¸ ç¢ºèªæ°¸ä¹…åˆªé™¤" if multi_lang=="ç¹é«”ä¸­æ–‡" else "âš¡ï¸ Confirm Permanent Delete", key=f"delete_topic_really_{topic}"):
            chat_file = get_chat_path(user_id, topic)
            try:
                if os.path.exists(chat_file):
                    os.remove(chat_file)
                    st.success(f"{T['delete_topic']}ï¼š{topic}")
                    st.session_state[delete_topic_key] = False
                    st.rerun()
                else:
                    st.warning("ä¸»é¡Œæª”æ¡ˆä¸å­˜åœ¨" if multi_lang=="ç¹é«”ä¸­æ–‡" else "Topic file not found")
            except Exception as e:
                st.error(f"åˆªé™¤ä¸»é¡Œå¤±æ•—ï¼š{e}" if multi_lang=="ç¹é«”ä¸­æ–‡" else f"Delete topic failed: {e}")


# Sidebar: å…¬å¸+PDF
if st.session_state.get("after_build_db"):
    st.session_state["sidebar_company_input"] = ""
    st.session_state["sidebar_pdf_uploader"] = None
    st.session_state["after_build_db"] = False

# ==== å…¬å¸PDFä¸Šå‚³ ====
st.sidebar.header(T["upload_pdf"])
company_name = st.sidebar.text_input(T["company_name"], value="", key="sidebar_company_input")
if not company_name.strip():
    st.sidebar.warning(T["pdf_upload_tip"])
    uploaded_file = None
else:
    uploaded_file = st.sidebar.file_uploader(T["upload_pdf"], type="pdf", key="sidebar_pdf_uploader")
    if uploaded_file is not None:
        now = datetime.now().strftime("%Y%m%d_%H%M%S")
        safe_name = safe_folder_name(company_name)
        pdf_filename = f"{safe_name}_{now}.pdf"
        pdf_path = os.path.join("data", pdf_filename)
        os.makedirs(os.path.dirname(pdf_path), exist_ok=True)
        with open(pdf_path, "wb") as f:
            f.write(uploaded_file.read())
        with st.spinner("å»ºæ§‹å‘é‡è³‡æ–™åº«ä¸­...ï¼ˆè«‹å‹¿é‡æ–°æ•´ç†æˆ–æ“ä½œï¼Œé è¨ˆ 10-30 ç§’ï¼‰" if multi_lang=="ç¹é«”ä¸­æ–‡" else "Building vector DB... Please wait."):
            try:
                result = subprocess.run(
                    [sys.executable, "create_db.py", "--pdf", pdf_path, "--company", company_name],
                    capture_output=True,
                    text=True,
                    check=False
                )
                if result.returncode == 0:
                    company_mapping[safe_name] = company_name
                    save_company_mapping(company_mapping)
                    waited = 0
                    while waited < 5:
                        new_map = load_company_mapping()
                        new_list = get_companies_list()
                        if safe_name in new_map and safe_name in new_list:
                            break
                        time.sleep(0.1)
                        waited += 0.1
                    if not (safe_name in new_map and safe_name in new_list):
                        st.warning("âš ï¸ å»ºåº«å®Œæˆä½†åŒæ­¥å»¶é²ï¼Œè«‹æ‰‹å‹•åˆ·æ–°é é¢" if multi_lang=="ç¹é«”ä¸­æ–‡" else "âš ï¸ Sync delay, please refresh the page manually.")
                    st.toast(f"âœ… {company_name} è³‡æ–™åº«å»ºç«‹å®Œæˆï¼" if multi_lang=="ç¹é«”ä¸­æ–‡" else f"âœ… {company_name} database created!", icon="âœ…")
                    st.session_state["company_selected"] = safe_name
                    # åªè¨­ flagï¼Œæ¸…ç©ºæ“ä½œåˆ°ä¸‹æ¬¡ rerun
                    st.session_state["after_build_db"] = True
                    st.rerun()
                else:
                    st.error(f"âŒ è³‡æ–™åº«å»ºç«‹å¤±æ•—ï¼š\n{result.stderr}" if multi_lang=="ç¹é«”ä¸­æ–‡" else f"âŒ DB build failed:\n{result.stderr}")
            except Exception as e:
                st.error(f"âŒ å»ºåº«åŸ·è¡Œå¤±æ•—ï¼š{e}" if multi_lang=="ç¹é«”ä¸­æ–‡" else f"âŒ Build process failed: {e}")


companies = get_companies_list()  
company_mapping = load_company_mapping() 

# ç”¨ mapping å–å¾—æ‰€æœ‰é¡¯ç¤ºç”¨ä¸­æ–‡åï¼ˆè‹¥ç„¡å°±ç”¨ safe_nameï¼‰
display_names = [company_mapping.get(code, code) for code in companies]
# åæŸ¥è¡¨ï¼ˆé¡¯ç¤ºå â†’ safe_nameï¼Œç¢ºä¿é¡¯ç¤ºåå”¯ä¸€å³å¯ï¼‰
display_to_code = {display: code for display, code in zip(display_names, companies)}

print("companies =", companies)
print("company_mapping =", company_mapping)
print("display_names =", display_names)

# æ ¹æ“š session_state è¨­å®šé è¨­ index
selected_safe_name = st.session_state.get("company_selected", None)
if selected_safe_name and selected_safe_name in companies:
    selected_display_name = company_mapping.get(selected_safe_name, selected_safe_name)
else:
    selected_display_name = None

if selected_display_name and selected_display_name in display_names:
    selected_index = display_names.index(selected_display_name) + 1
else:
    selected_index = 0

# é€™è£¡ sidebar ä¸‹æ‹‰ï¼Œé¡¯ç¤ºçš„åªæœ‰å…¬å¸ä¸­æ–‡å
company_display = st.sidebar.selectbox(
    T["select_company"],
    [T["select_company"]] + display_names,
    index=selected_index,
    key="company_selected_display"
)
if company_display == T["select_company"]:
    DB_FAISS_PATH = None
    company_selected = None
else:
    # é¸åˆ°çš„å…¬å¸ display name â†’ safe name
    company_selected = display_to_code[company_display]
    st.session_state["company_selected"] = company_selected
    DB_FAISS_PATH = os.path.join(VECTOR_DIR, company_selected, "db_faiss")

# å¤šèªåˆ‡æ›ï¼ˆé¡¯ç¤ºåœ¨æœ€ä¸Šé¢å·²ç¶“æœ‰ï¼Œä¸éœ€å†é‡è¤‡ï¼‰

# ====== é€²éšåŠŸèƒ½/èŠå¤©ç´€éŒ„æœå°‹/ä½¿ç”¨å»ºè­° ======
with st.sidebar.expander("ğŸ“Œ é€²éšåŠŸèƒ½" if multi_lang=="ç¹é«”ä¸­æ–‡" else "ğŸ“Œ Advanced"):
    st.markdown("ğŸ” **èŠå¤©ç´€éŒ„æœå°‹**" if multi_lang=="ç¹é«”ä¸­æ–‡" else "ğŸ” **Chat Log Search**")
    search_key = st.text_input(
        "è¼¸å…¥é—œéµå­—æœå°‹èŠå¤©ç´€éŒ„" if multi_lang=="ç¹é«”ä¸­æ–‡" else "Search keyword in chat logs", 
        key="search_history"
    )
    # æœå°‹ç¯„åœä¸‹æ‹‰é¸å–®ï¼ˆä¸­è‹±æ”¯æ´ï¼‰
    if multi_lang == "ç¹é«”ä¸­æ–‡":
        scope_options = ["ç›®å‰ä¸»é¡Œ", "æ‰€æœ‰ä¸»é¡Œ", "æ‰€æœ‰ç”¨æˆ¶"]
        scope_label = "æœå°‹ç¯„åœ"
    else:
        scope_options = ["Current topic", "All topics", "All users"]
        scope_label = "Scope"
    search_scope = st.selectbox(scope_label, scope_options, key="search_scope")

    # ä¸­è‹± mappingï¼Œæœå°‹æ™‚çµ±ä¸€ç”¨ä¸­æ–‡åšé‚è¼¯
    search_scope_mapping = {
        "ç›®å‰ä¸»é¡Œ": "ç›®å‰ä¸»é¡Œ", "Current topic": "ç›®å‰ä¸»é¡Œ",
        "æ‰€æœ‰ä¸»é¡Œ": "æ‰€æœ‰ä¸»é¡Œ", "All topics": "æ‰€æœ‰ä¸»é¡Œ",
        "æ‰€æœ‰ç”¨æˆ¶": "æ‰€æœ‰ç”¨æˆ¶", "All users": "æ‰€æœ‰ç”¨æˆ¶"
    }
    search_results = []

    if search_key:
        scope_internal = search_scope_mapping.get(search_scope, "ç›®å‰ä¸»é¡Œ")
        if scope_internal == "ç›®å‰ä¸»é¡Œ":
            history = load_chat(user_id, topic)
            for m in history:
                if search_key in m["content"]:
                    search_results.append({
                        "user": user_id,
                        "topic": topic,
                        "role": m["role"],
                        "content": m["content"]
                    })
        elif scope_internal == "æ‰€æœ‰ä¸»é¡Œ":
            topics = list_topics(user_id)
            for t in topics:
                history = load_chat(user_id, t)
                for m in history:
                    if search_key in m["content"]:
                        search_results.append({
                            "user": user_id,
                            "topic": t,
                            "role": m["role"],
                            "content": m["content"]
                        })
        elif scope_internal == "æ‰€æœ‰ç”¨æˆ¶":
            users = list_users()
            for u in users:
                topics = list_topics(u)
                for t in topics:
                    history = load_chat(u, t)
                    for m in history:
                        if search_key in m["content"]:
                            search_results.append({
                                "user": u,
                                "topic": t,
                                "role": m["role"],
                                "content": m["content"]
                            })

        st.markdown(
            f"å…±æ‰¾åˆ° <span style='color:orange;font-weight:bold'>{len(search_results)}</span> ç­†ï¼š" 
            if multi_lang=="ç¹é«”ä¸­æ–‡" 
            else f"Found <span style='color:orange;font-weight:bold'>{len(search_results)}</span> record(s):", 
            unsafe_allow_html=True
        )
        for i, m in enumerate(search_results):
            st.markdown(
                f"<b>{m['user']}/{m['topic']}</b> | {m['role']}ï¼š{m['content'][:100]}{'...' if len(m['content'])>100 else ''}",
                unsafe_allow_html=True
            )
    st.caption(
        "å¯é¸æœå°‹ã€Œç›®å‰ä¸»é¡Œã€ã€ã€Œæ‰€æœ‰ä¸»é¡Œã€ã€ã€Œæ‰€æœ‰ç”¨æˆ¶ã€" if multi_lang=="ç¹é«”ä¸­æ–‡" 
        else "You can search in current topic / all topics / all users"
    )

st.sidebar.info(
    """  
ğŸ’¡ ä½¿ç”¨å»ºè­°ï¼š
- å…ˆä¸Šå‚³å…¬å¸è²¡å ± PDFï¼Œè‡ªå‹•å»ºç«‹è³‡æ–™åº«
- å¯åˆ‡æ›å…¬å¸/èªè¨€ï¼Œè‡ªç”±æŸ¥è©¢
- å¯åˆ†ä¸»é¡Œç®¡ç†å°è©±èˆ‡æ­·å²æŸ¥è©¢
- æå•å¾Œå¯ç”¨ /åˆ†æ é€²è¡Œæƒ…ç·’é æ¸¬
- å›è¦†å…§å®¹æ›´è²¼è¿‘ AI å£å»ï¼Œè‡ªç„¶ç°¡æ˜
""" if multi_lang=="ç¹é«”ä¸­æ–‡" else
    """  
ğŸ’¡ Suggestions:
- Upload company financial PDF first to build the database
- Switch company/language freely for queries
- Manage topics and chat history
- Use /analyze for sentiment analysis
- Answers are now more conversational and AI-like
""")

tab1, tab2, tab3 = st.tabs([T["tab1"], T["tab2"], T["tab3"]])

# <<<< åˆå§‹åŒ– session_state["messages"]
if "messages" not in st.session_state:
    try:
        st.session_state["messages"] = load_chat(user_id, topic)
    except Exception:
        st.session_state["messages"] = []

# ========== åˆ†é 1ï¼šAIå•ç­” ==========

# åˆå§‹åŒ–åˆ†é 1çš„èŠå¤©ç´€éŒ„
if "messages_tab1" not in st.session_state:
    try:
        st.session_state["messages_tab1"] = load_chat(user_id, topic)
    except Exception:
        st.session_state["messages_tab1"] = []

with tab1:
    st.title(T["chat_title"])
    st.markdown(
        """è«‹è¼¸å…¥æ‚¨çš„å•é¡Œï¼ˆä¸­è‹±æ–‡çš†å¯ï¼‰ï¼Œä¾‹å¦‚ï¼š\n- å°ç©é›» 2024 å¹´ç¬¬ä¸€å­£çš„ç‡Ÿæ¥­æ”¶å…¥æ˜¯å¤šå°‘ï¼Ÿ\n- What is TSMC's Q1 2024 revenue?\n- /åˆ†æ é æœŸæœªä¾†æŠ˜èˆŠè²»ç”¨å°‡ä¸Šå‡"""
        if multi_lang=="ç¹é«”ä¸­æ–‡" else
        """Ask your question (either language), e.g.:\n- What is TSMC's Q1 2024 revenue?\n- å°ç©é›» 2024 å¹´ç¬¬ä¸€å­£çš„ç‡Ÿæ¥­æ”¶å…¥æ˜¯å¤šå°‘ï¼Ÿ\n- /analyze Depreciation expense is expected to rise"""
    )

    if st.button(T["clear_chat"], key="clear_chat"):
        st.session_state["messages_tab1"] = []
        save_chat(st.session_state["messages_tab1"], user_id, topic)
        st.rerun()

    for chat in st.session_state["messages_tab1"]:
        with st.chat_message(chat["role"]):
            st.markdown(chat["content"], unsafe_allow_html=True)

user_input = st.chat_input(T["chat_input"], key="ai_chat")
if user_input:
    st.session_state["messages_tab1"].append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)
    with st.spinner("AI æ­£åœ¨æŸ¥è©¢ ..." if multi_lang == "ç¹é«”ä¸­æ–‡" else "AI is searching ..."):
        try:
            reply = integrated_ai_summary(user_input, DB_FAISS_PATH, multi_lang)
            # ==é€™è£¡åŸæœ¬æœ‰å…¬å¸æŸ¥æ ¸è­¦èªå€å¡Šï¼Œç›´æ¥ç§»é™¤==
        except Exception as e:
            reply = f"âŒ æŸ¥è©¢å¤±æ•—ï¼š{e}" if multi_lang == "ç¹é«”ä¸­æ–‡" else f"âŒ Error: {e}"

    st.session_state["messages_tab1"].append({"role": "assistant", "content": reply})
    with st.chat_message("assistant"):
        st.markdown(reply, unsafe_allow_html=True)
    save_chat(st.session_state["messages_tab1"], user_id, topic)
    st.toast("å·²å®Œæˆå¤šä¾†æºæŸ¥è©¢" if multi_lang == "ç¹é«”ä¸­æ–‡" else "Multi-source search complete", icon="ğŸ¤–")


# ========== åˆ†é 2ï¼šè²¡å ±æ‘˜è¦ ==========
with tab2:
    st.title(T["tab2_title"])
    st.markdown(T["tab2_markdown"])

    input_text = st.text_area(
        T["tab2_input_placeholder"], ""
    )

    if st.button(T["tab2_analyze_btn2"]):
        if not input_text.strip():
            st.warning(T["tab2_enter_content"])
        else:
            with st.spinner(T["tab2_spinner_analyze"]):
                # æ¢åˆ—ä¸‰å€‹é‡é»
                summary_prompt = (
                    "è«‹é–±è®€ä¸‹åˆ—å…§å®¹ï¼Œæ¢åˆ—ä¸‰å€‹ç¶“ç‡Ÿé‡é»ï¼ˆæ¯é»15å­—å…§ï¼Œä¸è¦æŠ„åŸæ–‡ï¼‰ï¼š\n\n"
                    if multi_lang == "ç¹é«”ä¸­æ–‡"
                    else "Read the following and list three business highlights (no more than 15 words each, do not copy the original):\n\n"
                ) + input_text
                summary = ask_openai(summary_prompt, lang=multi_lang)

                # åˆ¤æ–·æƒ…ç·’
                sentiment_prompt = (
                    "è«‹é–±è®€ä¸‹åˆ—å…§å®¹ï¼Œåˆ¤æ–·ç¶“ç‡Ÿæƒ…ç·’ä¸¦åªå›è¦†ä¸€å€‹è©ï¼ˆæ¨‚è§€/ä¸­æ€§/ä¿å®ˆï¼‰ï¼š\n\n"
                    if multi_lang == "ç¹é«”ä¸­æ–‡"
                    else "Read the following, judge the management sentiment, and only reply with one word (Optimistic/Neutral/Conservative):\n\n"
                ) + input_text
                sentiment = ask_openai(sentiment_prompt, lang=multi_lang)

                # é¡¯ç¤º
                st.markdown(f"#### {T['tab2_key_summary']}")
                st.markdown(summary)
                st.markdown(
                    f"#### {T['tab2_sentiment']}<span style='color:orange;font-weight:bold'>{sentiment}</span>",
                    unsafe_allow_html=True
                )

with tab3:
    st.title(T["tab3"])
    st.markdown(T["tab3_title"])
    indicator_options = ["ç‡Ÿæ”¶", "æ·¨åˆ©", "EPS", "æ¯›åˆ©ç‡"] if multi_lang == "ç¹é«”ä¸­æ–‡" else ["Revenue", "Net Income", "EPS", "Gross Margin"]

    st.subheader(T["tab3_multi_title"])
    selected_companies_display = st.multiselect(
        T["tab3_multi_company"],
        display_names,
        default=[display_names[0]] if display_names else [],
        key="tab3_multi_company"
    )
    selected_companies = [display_to_code[d] for d in selected_companies_display]
    compare_indicator = st.selectbox(
        T["tab3_multi_indicator"],
        indicator_options,
        index=0,
        key="compare_indicator"
    )
    year_range = st.slider(
        T["tab3_year"], 
        min_value=2015, 
        max_value=datetime.now().year, 
        value=(2020, datetime.now().year), 
        key="tab3_year_multi"
    )

    btn_col1, btn_col2 = st.columns([1, 1])
    with btn_col1:
        compare_btn_clicked = st.button(T["tab3_multi_btn"], key="compare_btn", use_container_width=True)
    with btn_col2:
        delete_btn_clicked = st.button("ğŸ—‘ï¸", key="delete_chart_btn", use_container_width=True, help="åˆªé™¤åœ–è¡¨")

    if "multi_chart_df" not in st.session_state:
        st.session_state["multi_chart_df"] = None

    if compare_btn_clicked:
        if not selected_companies:
            st.warning(T["tab3_multi_no_company"])
            st.session_state["multi_chart_df"] = None
        else:
            with st.spinner(T["tab3_multi_spinner"]):
                selected_display_names = [company_mapping.get(code, code) for code in selected_companies]
                years = list(range(year_range[0], year_range[1]+1))
                # é€™è£¡ç”¨ä¸­æ–‡æ¬„ä½æŸ¥è©¢
                indicator_map = {
                    "Revenue": "ç‡Ÿæ”¶", "Net Income": "æ·¨åˆ©", "EPS": "EPS", "Gross Margin": "æ¯›åˆ©ç‡",
                    "ç‡Ÿæ”¶": "ç‡Ÿæ”¶", "æ·¨åˆ©": "æ·¨åˆ©", "æ¯›åˆ©ç‡": "æ¯›åˆ©ç‡"
                }
                indicator_key = indicator_map.get(compare_indicator, compare_indicator)
                df = get_company_year_data(selected_display_names, indicator_key, years)
                # å¦‚æœä¸æ˜¯ç™¾åˆ†æ¯”å†é™¤ä»¥ 1e8
                if not df.empty and "%" not in indicator_key:
                    df[indicator_key] = df[indicator_key] / 1e8
                st.session_state["multi_chart_df"] = df

    if delete_btn_clicked:
        st.session_state["multi_chart_df"] = None

    df = st.session_state.get("multi_chart_df", None)
    if df is not None and not df.empty:
        # é€™è£¡ indicator_key ä¹Ÿè¦ç”¨
        indicator_map = {
            "Revenue": "ç‡Ÿæ”¶", "Net Income": "æ·¨åˆ©", "EPS": "EPS", "Gross Margin": "æ¯›åˆ©ç‡",
            "ç‡Ÿæ”¶": "ç‡Ÿæ”¶", "æ·¨åˆ©": "æ·¨åˆ©", "æ¯›åˆ©ç‡": "æ¯›åˆ©ç‡"
        }
        indicator_key = indicator_map.get(compare_indicator, compare_indicator)

        chart_col, slider_col = st.columns([5, 1])
        with slider_col:
            chart_width = st.slider("å¯¬åº¦", min_value=2.0, max_value=8.0, value=3.2, step=0.1, key="chart_width_multi")
            chart_height = st.slider("é«˜åº¦", min_value=1.0, max_value=5.0, value=2.0, step=0.1, key="chart_height_multi")
        with chart_col:
            fig, ax = plt.subplots(figsize=(chart_width, chart_height))
            for company in df["å…¬å¸"].unique():
                df_c = df[df["å…¬å¸"] == company].sort_values("å¹´åº¦")
                label = f"{company} ({df_c['ä¾†æº'].iloc[0]})"
                ax.plot(df_c["å¹´åº¦"], df_c[indicator_key], marker="o", label=label)
            # æ¨™é¡Œ/è»¸/åœ–ä¾‹éƒ½ç”¨ prop
            if prop:
                if "%" in indicator_key or indicator_key in ["æ¯›åˆ©ç‡", "Gross Margin"]:
                    ax.set_ylabel(indicator_key + ("ï¼ˆ%ï¼‰" if multi_lang == "ç¹é«”ä¸­æ–‡" else " (%)"), fontproperties=prop)
                else:
                    ax.set_ylabel(indicator_key + ("ï¼ˆå„„å…ƒï¼‰" if multi_lang == "ç¹é«”ä¸­æ–‡" else " (100M)"), fontproperties=prop)
                ax.set_xlabel("å¹´åº¦" if multi_lang == "ç¹é«”ä¸­æ–‡" else "Year", fontproperties=prop)
                ax.set_title(T["tab3_chart_title"].format(indicator=compare_indicator), fontsize=14, fontproperties=prop)
                ax.legend(
                    fontsize=8,
                    bbox_to_anchor=(1.01, 0.5),
                    loc='center left',
                    borderaxespad=0.,
                    prop=prop
                )
            else:
                if "%" in indicator_key or indicator_key in ["æ¯›åˆ©ç‡", "Gross Margin"]:
                    ax.set_ylabel(indicator_key + ("ï¼ˆ%ï¼‰" if multi_lang == "ç¹é«”ä¸­æ–‡" else " (%)"))
                else:
                    ax.set_ylabel(indicator_key + ("ï¼ˆå„„å…ƒï¼‰" if multi_lang == "ç¹é«”ä¸­æ–‡" else " (100M)"))
                ax.set_xlabel("å¹´åº¦" if multi_lang == "ç¹é«”ä¸­æ–‡" else "Year")
                ax.set_title(T["tab3_chart_title"].format(indicator=compare_indicator), fontsize=14)
                ax.legend(
                    fontsize=8,
                    bbox_to_anchor=(1.01, 0.5),
                    loc='center left',
                    borderaxespad=0.
                )
            ax.grid(True)
            st.pyplot(fig, use_container_width=False)
            st.write(df)
    elif df is not None and df.empty:
        st.warning(T["tab3_multi_no_data"])